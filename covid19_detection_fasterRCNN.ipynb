{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1R7sy6XgoTcZRtQSUdI53nXAG4rx_BG1z",
      "authorship_tag": "ABX9TyPjXI05KixKkPTNiARpAPUA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoharu/DATA-AI/blob/main/covid19_detection_fasterRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "WO7uDDBWtbYY"
      },
      "outputs": [],
      "source": [
        "def get_train_file_path(image_id):\n",
        "    return \"/content/drive/MyDrive/JBNU/2023 AI 전공특화교육/project/slim-covid 19-detection/resized_archive/train\".format(image_id)\n",
        "\n",
        "def get_test_file_path(image_id):\n",
        "    return \"/content/drive/MyDrive/JBNU/2023 AI 전공특화교육/project/slim-covid 19-detection/resized_archive/test\".format(image_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "updated_train_labels = pd.read_csv('/content/drive/MyDrive/JBNU/2023 AI 전공특화교육/project/slim-covid 19-detection/updated_train_labels.csv/updated_train_labels.csv')\n",
        "\n",
        "updated_train_labels['jpg_path'] = updated_train_labels['id'].apply(get_train_file_path)\n",
        "train = updated_train_labels.copy()\n",
        "display(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "2nSCQq2ZtlhK",
        "outputId": "8872da58-9eb7-4720-9e8d-e5a192743f49"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             id                                           dcm_path  \\\n",
              "0  000a312787f2  /kaggle/input/siim-covid19-detection/train/577...   \n",
              "1  000a312787f2  /kaggle/input/siim-covid19-detection/train/577...   \n",
              "2  000c3a3f293f  /kaggle/input/siim-covid19-detection/train/ff0...   \n",
              "3  0012ff7358bc  /kaggle/input/siim-covid19-detection/train/9d5...   \n",
              "4  0012ff7358bc  /kaggle/input/siim-covid19-detection/train/9d5...   \n",
              "\n",
              "         xmin       ymin        xmax        ymax human_label  negative  \\\n",
              "0  2245.91208  591.20528  3340.57370  2352.75472     typical         0   \n",
              "1   789.28836  582.43035  1815.94498  2499.73327     typical         0   \n",
              "2     0.00000    0.00000     1.00000     1.00000    negative         1   \n",
              "3   677.42216  197.97662  1545.21983  1197.75876     typical         0   \n",
              "4  1792.69064  402.55250  2409.71798  1606.91050     typical         0   \n",
              "\n",
              "   typical  indeterminate  ...  ImageInstanceUID     img_shape width height  \\\n",
              "0        1              0  ...      000a312787f2  (3488, 4256)  4256   3488   \n",
              "1        1              0  ...      000a312787f2  (3488, 4256)  4256   3488   \n",
              "2        0              0  ...      000c3a3f293f  (2320, 2832)  2832   2320   \n",
              "3        1              0  ...      0012ff7358bc  (2544, 3056)  3056   2544   \n",
              "4        1              0  ...      0012ff7358bc  (2544, 3056)  3056   2544   \n",
              "\n",
              "  frac_xmin  frac_xmax  frac_ymin  frac_ymax  integer_label  \\\n",
              "0  0.527705   0.784909   0.169497   0.674528              3   \n",
              "1  0.185453   0.426679   0.166981   0.716667              3   \n",
              "2  0.000000   0.000353   0.000000   0.000431              2   \n",
              "3  0.221670   0.505635   0.077821   0.470817              3   \n",
              "4  0.586613   0.788520   0.158236   0.631647              3   \n",
              "\n",
              "                                            jpg_path  \n",
              "0  /content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...  \n",
              "1  /content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...  \n",
              "2  /content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...  \n",
              "3  /content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...  \n",
              "4  /content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c8d2ad4-e83f-40c9-8cda-3f1a24dab8b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dcm_path</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>human_label</th>\n",
              "      <th>negative</th>\n",
              "      <th>typical</th>\n",
              "      <th>indeterminate</th>\n",
              "      <th>...</th>\n",
              "      <th>ImageInstanceUID</th>\n",
              "      <th>img_shape</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>frac_xmin</th>\n",
              "      <th>frac_xmax</th>\n",
              "      <th>frac_ymin</th>\n",
              "      <th>frac_ymax</th>\n",
              "      <th>integer_label</th>\n",
              "      <th>jpg_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000a312787f2</td>\n",
              "      <td>/kaggle/input/siim-covid19-detection/train/577...</td>\n",
              "      <td>2245.91208</td>\n",
              "      <td>591.20528</td>\n",
              "      <td>3340.57370</td>\n",
              "      <td>2352.75472</td>\n",
              "      <td>typical</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>000a312787f2</td>\n",
              "      <td>(3488, 4256)</td>\n",
              "      <td>4256</td>\n",
              "      <td>3488</td>\n",
              "      <td>0.527705</td>\n",
              "      <td>0.784909</td>\n",
              "      <td>0.169497</td>\n",
              "      <td>0.674528</td>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000a312787f2</td>\n",
              "      <td>/kaggle/input/siim-covid19-detection/train/577...</td>\n",
              "      <td>789.28836</td>\n",
              "      <td>582.43035</td>\n",
              "      <td>1815.94498</td>\n",
              "      <td>2499.73327</td>\n",
              "      <td>typical</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>000a312787f2</td>\n",
              "      <td>(3488, 4256)</td>\n",
              "      <td>4256</td>\n",
              "      <td>3488</td>\n",
              "      <td>0.185453</td>\n",
              "      <td>0.426679</td>\n",
              "      <td>0.166981</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000c3a3f293f</td>\n",
              "      <td>/kaggle/input/siim-covid19-detection/train/ff0...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>000c3a3f293f</td>\n",
              "      <td>(2320, 2832)</td>\n",
              "      <td>2832</td>\n",
              "      <td>2320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0012ff7358bc</td>\n",
              "      <td>/kaggle/input/siim-covid19-detection/train/9d5...</td>\n",
              "      <td>677.42216</td>\n",
              "      <td>197.97662</td>\n",
              "      <td>1545.21983</td>\n",
              "      <td>1197.75876</td>\n",
              "      <td>typical</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0012ff7358bc</td>\n",
              "      <td>(2544, 3056)</td>\n",
              "      <td>3056</td>\n",
              "      <td>2544</td>\n",
              "      <td>0.221670</td>\n",
              "      <td>0.505635</td>\n",
              "      <td>0.077821</td>\n",
              "      <td>0.470817</td>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0012ff7358bc</td>\n",
              "      <td>/kaggle/input/siim-covid19-detection/train/9d5...</td>\n",
              "      <td>1792.69064</td>\n",
              "      <td>402.55250</td>\n",
              "      <td>2409.71798</td>\n",
              "      <td>1606.91050</td>\n",
              "      <td>typical</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0012ff7358bc</td>\n",
              "      <td>(2544, 3056)</td>\n",
              "      <td>3056</td>\n",
              "      <td>2544</td>\n",
              "      <td>0.586613</td>\n",
              "      <td>0.788520</td>\n",
              "      <td>0.158236</td>\n",
              "      <td>0.631647</td>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/JBNU/2023 AI 전공특ᄒ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c8d2ad4-e83f-40c9-8cda-3f1a24dab8b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c8d2ad4-e83f-40c9-8cda-3f1a24dab8b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c8d2ad4-e83f-40c9-8cda-3f1a24dab8b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8828be47-8026-4d0e-84fb-ae40f1ddaf97\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8828be47-8026-4d0e-84fb-ae40f1ddaf97')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8828be47-8026-4d0e-84fb-ae40f1ddaf97 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries"
      ],
      "metadata": {
        "id": "4EQR35tvu1Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm_notebook as tqdm # progress bar\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "\n",
        "# torchvision\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# CV\n",
        "import cv2\n",
        "\n",
        "# Albumenatations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "#from pycocotools.coco import COCO\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# glob\n",
        "from glob import glob\n",
        "\n",
        "# numba\n",
        "import numba\n",
        "from numba import jit\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings."
      ],
      "metadata": {
        "id": "EVmnDRXwu3Jj"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CFG"
      ],
      "metadata": {
        "id": "paCIsgZIu9wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DefaultConfig:\n",
        "    n_folds: int = 5\n",
        "    seed: int = 2021\n",
        "    num_classes: int = 4 # \"negative\", \"typical\", \"indeterminate\", \"atypical\"\n",
        "    img_size: int = 256\n",
        "    fold_num: int = 0\n",
        "    device: str = 'cuda:0'"
      ],
      "metadata": {
        "id": "GZGo2L7qvBYn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(DefaultConfig.device) if torch.cuda.is_available() else torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "rjpR_c0RvCJJ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Choose your optimizers:\n",
        "Adam = False\n",
        "if Adam:\n",
        "    Adam_config = {\"lr\" : 0.001, \"betas\" : (0.9, 0.999), \"eps\" : 1e-08}\n",
        "else:\n",
        "    SGD_config = {\"lr\" : 0.001, \"momentum\" : 0.9, \"weight_decay\" : 0.001}"
      ],
      "metadata": {
        "id": "l3MdGrcnvGPn"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(DefaultConfig.seed)"
      ],
      "metadata": {
        "id": "QLg3Bp4XvLCM"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split"
      ],
      "metadata": {
        "id": "8TY5hoA4vOps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_folds = train.copy()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=DefaultConfig.seed)\n",
        "for n, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds.integer_label)):\n",
        "    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = int(n)\n",
        "df_folds['fold'] = df_folds['fold'].astype(int)\n",
        "print(df_folds.groupby(['fold', df_folds.integer_label]).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1F4rN7NvRns",
        "outputId": "808d92e3-deba-4a5a-8a75-59d1587947ff"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold  integer_label\n",
            "0     0                 126\n",
            "      1                 299\n",
            "      2                 347\n",
            "      3                1207\n",
            "1     0                 126\n",
            "      1                 299\n",
            "      2                 347\n",
            "      3                1207\n",
            "2     0                 126\n",
            "      1                 299\n",
            "      2                 347\n",
            "      3                1207\n",
            "3     0                 126\n",
            "      1                 298\n",
            "      2                 347\n",
            "      3                1207\n",
            "4     0                 125\n",
            "      1                 299\n",
            "      2                 348\n",
            "      3                1206\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# albumentaion"
      ],
      "metadata": {
        "id": "9DXChEPwzFvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.OneOf([\n",
        "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2,\n",
        "                                 val_shift_limit=0.2, p=0.3),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2,\n",
        "                                       contrast_limit=0.2, p=0.3),\n",
        "        ], p=0.2),\n",
        "        A.Resize(height=DefaultConfig.img_size, width=DefaultConfig.img_size, p=1.0),\n",
        "        #A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ],\n",
        "    p=1.0, bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(height=DefaultConfig.img_size, width=DefaultConfig.img_size, p=1.0),\n",
        "        #A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "metadata": {
        "id": "fcV1DmBNzHLI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset & dataloader"
      ],
      "metadata": {
        "id": "vYaTMQ53zqfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_ids, df, transforms=None, test=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_ids = image_ids\n",
        "        self.df = df\n",
        "        self.file_names = df['jpg_path'].values\n",
        "        self.transforms = transforms\n",
        "        self.test = test\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image_id = self.image_ids[index]\n",
        "\n",
        "        image, boxes, labels = self.load_image_and_boxes(index)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = torch.tensor(labels)\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "\n",
        "        if self.transforms:\n",
        "            for i in range(10):\n",
        "                sample = self.transforms(**{\n",
        "                    'image': image,\n",
        "                    'bboxes': target['boxes'],\n",
        "                    'labels': labels\n",
        "                })\n",
        "                if len(sample['bboxes']) > 0:\n",
        "                    image = sample['image']\n",
        "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "                    break\n",
        "        return image, target, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]\n",
        "\n",
        "\n",
        "    def load_image_and_boxes(self, index):\n",
        "        image_id = self.image_ids[index]\n",
        "        image = cv2.imread(self.file_names[index], cv2.IMREAD_COLOR).copy().astype(np.float32)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        records = self.df[self.df['id'] == image_id]\n",
        "        boxes = []\n",
        "        for bbox in records[['frac_xmin', 'frac_ymin', 'frac_xmax', 'frac_ymax']].values:\n",
        "            bbox = np.clip(bbox, 0, 1.0)\n",
        "            temp = A.convert_bbox_from_albumentations(bbox, 'pascal_voc', image.shape[0], image.shape[0])\n",
        "            boxes.append(temp)\n",
        "        '''\n",
        "        [0: 'atypical', 1: 'indeterminate', 2: 'negative', 3: 'typical']\n",
        "        '''\n",
        "        labels = records['integer_label'].values\n",
        "        return image, boxes, labels"
      ],
      "metadata": {
        "id": "sfuJCl1LzvCm"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_folds = df_folds.set_index('id')\n",
        "\n",
        "def get_train_dataset(fold_number):\n",
        "    return CustomDataset(\n",
        "        image_ids = df_folds[df_folds['fold'] != fold_number].index.values,\n",
        "        df = train,\n",
        "        transforms = get_train_transforms()\n",
        "    )\n",
        "\n",
        "def get_validation_dataset(fold_number):\n",
        "    return CustomDataset(\n",
        "        image_ids = df_folds[df_folds['fold'] == fold_number].index.values,\n",
        "        df = train,\n",
        "        transforms = get_valid_transforms()\n",
        "    )\n",
        "\n",
        "def get_train_data_loader(train_dataset, batch_size=16):\n",
        "    return DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = False,\n",
        "        num_workers = 4,\n",
        "        collate_fn = collate_fn\n",
        "    )\n",
        "\n",
        "def get_validation_data_loader(valid_dataset, batch_size=16):\n",
        "    return DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = False,\n",
        "        num_workers = 4,\n",
        "        collate_fn = collate_fn\n",
        "    )\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "215VokuHzLu1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show one image"
      ],
      "metadata": {
        "id": "iM2jXEcSzUgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    train_dataset = get_train_dataset(0)\n",
        "    image, target, image_id = train_dataset[2]\n",
        "    boxes = target['boxes'].cpu().numpy().astype(np.int32)\n",
        "    numpy_image = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "    for box in boxes:\n",
        "        cv2.rectangle(numpy_image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(numpy_image)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqe1F_LrzV-_",
        "outputId": "9dd93c8c-a816-44e8-fdaf-a0e5f9d8edb7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: 'NoneType' object has no attribute 'copy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show images"
      ],
      "metadata": {
        "id": "nBKKKNEj0KKL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_rows = 4\n",
        "n_cols = 4\n",
        "\n",
        "try:      # create train dataset and data-loader\n",
        "    train_dataset = get_train_dataset(fold_number=DefaultConfig.fold_num)\n",
        "    train_data_loader = get_train_data_loader(train_dataset, batch_size=16)\n",
        "\n",
        "    images, targets, image_ids = next(iter(train_data_loader))\n",
        "\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    # plot some augmentations!\n",
        "    fig, ax = plt.subplots(figsize=(20, 20), nrows=n_rows, ncols=n_cols)\n",
        "    for i in range(n_rows * n_cols):\n",
        "        boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
        "        sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        for box in boxes:\n",
        "            cv2.rectangle(sample,\n",
        "                          (box[0], box[1]),\n",
        "                          (box[2], box[3]),\n",
        "                          (255, 0, 0), 3)\n",
        "\n",
        "        ax[i // n_rows][i % n_cols].imshow(sample)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoJ1Gtd54mxl",
        "outputId": "76a0bbd7-01c3-4545-d155-9dcd61c1667d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: Caught AttributeError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"<ipython-input-72-c97abc195603>\", line 15, in __getitem__\n",
            "    image, boxes, labels = self.load_image_and_boxes(index)\n",
            "  File \"<ipython-input-72-c97abc195603>\", line 41, in load_image_and_boxes\n",
            "    image = cv2.imread(self.file_names[index], cv2.IMREAD_COLOR).copy().astype(np.float32)\n",
            "AttributeError: 'NoneType' object has no attribute 'copy'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fitter"
      ],
      "metadata": {
        "id": "bioc3h0R5yhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "iou_thresholds = [0.5]\n",
        "\n",
        "class EvalMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.image_precision = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, gt_boxes, pred_boxes, n=1):\n",
        "        \"\"\" pred_boxes : need to be sorted.\"\"\"\n",
        "\n",
        "        self.image_precision = calculate_image_precision(pred_boxes,\n",
        "                                                         gt_boxes,\n",
        "                                                         thresholds=iou_thresholds,\n",
        "                                                         form='pascal_voc')\n",
        "        self.count += n\n",
        "        self.sum += self.image_precision * n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "g5tr4x1J50vu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Fitter:\n",
        "    def __init__(self, model, device, config):\n",
        "        self.config = config\n",
        "        self.epoch = 0\n",
        "\n",
        "        self.base_dir = f'./{config.folder}'\n",
        "        if not os.path.exists(self.base_dir):\n",
        "            os.makedirs(self.base_dir)\n",
        "\n",
        "        self.log_path = f'{self.base_dir}/log.txt'\n",
        "        self.best_summary_loss = 10**5\n",
        "        self.best_score = 0\n",
        "\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        # get the configured optimizer\n",
        "        if Adam:\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), **Adam_config)\n",
        "        else:\n",
        "            self.optimizer = torch.optim.SGD(self.model.parameters(), **SGD_config)\n",
        "\n",
        "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
        "        self.log(f'Fitter prepared. Device is {self.device}')\n",
        "        self.log(f'Fold num is {DefaultConfig.fold_num}')\n",
        "\n",
        "    def fit(self, train_loader, validation_loader):\n",
        "        for e in range(self.config.n_epochs):\n",
        "            if self.config.verbose:\n",
        "                lr = self.optimizer.param_groups[0]['lr']\n",
        "                timestamp = datetime.utcnow().isoformat()\n",
        "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
        "\n",
        "            t = time.time()\n",
        "            summary_loss = self.train_one_epoch(train_loader)\n",
        "\n",
        "            if e == 0:\n",
        "                self.best_summary_loss = summary_loss.avg\n",
        "\n",
        "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
        "\n",
        "            t = time.time()\n",
        "            _, eval_scores  = self.validation(validation_loader)\n",
        "\n",
        "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, image_precision: {eval_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "\n",
        "            #if summary_loss.avg < self.best_summary_loss:\n",
        "            if eval_scores.avg > self.best_score:\n",
        "                self.best_summary_loss = summary_loss.avg\n",
        "                self.best_score = eval_scores.avg\n",
        "                self.model.eval()\n",
        "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
        "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
        "                    os.remove(path)\n",
        "\n",
        "            if self.config.validation_scheduler:\n",
        "                self.scheduler.step(metrics=eval_scores.avg)\n",
        "                #self.scheduler.step(metrics=summary_loss.avg)\n",
        "\n",
        "            self.epoch += 1\n",
        "\n",
        "    def validation(self, val_loader):\n",
        "        self.model.eval()\n",
        "\n",
        "        # model.eval() mode --> it will return boxes and scores.\n",
        "        # in this part, just print train_loss\n",
        "        summary_loss = AverageMeter()\n",
        "        summary_loss.update(self.best_summary_loss, self.config.batch_size)\n",
        "\n",
        "        eval_scores = EvalMeter()\n",
        "        validation_image_precisions = []\n",
        "\n",
        "        t = time.time()\n",
        "        for step, (images, targets, image_ids) in enumerate(val_loader):\n",
        "            if self.config.verbose:\n",
        "                if step % self.config.verbose_step == 0:\n",
        "                    print(\n",
        "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
        "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
        "                        f'val_precision: {eval_scores.avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
        "                    )\n",
        "            with torch.no_grad():\n",
        "                images = torch.stack(images)\n",
        "                batch_size = images.shape[0]\n",
        "                images = images.to(self.device).float()\n",
        "                labels = [target['labels'].float() for target in targets]\n",
        "\n",
        "                \"\"\"\n",
        "                In model.train() mode, model(images)  is returning losses.\n",
        "                We are using model.eval() mode --> it will return boxes and scores.\n",
        "                \"\"\"\n",
        "                outputs = self.model(images)\n",
        "\n",
        "                for i, image in enumerate(images):\n",
        "                    gt_boxes = targets[i]['boxes'].data.cpu().numpy()\n",
        "                    boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
        "                    scores = outputs[i]['scores'].detach().cpu().numpy()\n",
        "\n",
        "                    preds_sorted_idx = np.argsort(scores)[::-1]\n",
        "                    preds_sorted_boxes = boxes[preds_sorted_idx]\n",
        "\n",
        "                    eval_scores.update(pred_boxes=preds_sorted_boxes, gt_boxes=gt_boxes)\n",
        "\n",
        "        return summary_loss, eval_scores\n",
        "\n",
        "    def train_one_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        summary_loss = AverageMeter()\n",
        "\n",
        "        t = time.time()\n",
        "        for step, (images, targets, image_ids) in enumerate(train_loader):\n",
        "            if self.config.verbose:\n",
        "                if step % self.config.verbose_step == 0:\n",
        "                    print(\n",
        "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
        "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
        "                    )\n",
        "\n",
        "            images = torch.stack(images)\n",
        "            images = images.to(self.device).float()\n",
        "            batch_size = images.shape[0]\n",
        "            boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
        "            labels = [target['labels'].to(self.device).float() for target in targets]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(images, targets)\n",
        "\n",
        "            loss = sum(loss for loss in outputs.values())\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            summary_loss.update(loss.detach().item(), batch_size)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.config.step_scheduler:\n",
        "                self.scheduler.step()\n",
        "\n",
        "        return summary_loss\n",
        "\n",
        "    def save(self, path):\n",
        "        self.model.eval()\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(), #'model_state_dict': self.model.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'best_summary_loss': self.best_summary_loss,\n",
        "            'epoch': self.epoch,\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
        "        self.epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    def log(self, message):\n",
        "        if self.config.verbose:\n",
        "            print(message)\n",
        "        with open(self.log_path, 'a+') as logger:\n",
        "            logger.write(f'{message}\\n')"
      ],
      "metadata": {
        "id": "BQr95s4U56gh"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainGlobalConfig:\n",
        "    num_workers: int = 4\n",
        "    batch_size: int = 16\n",
        "    n_epochs: int = 2 #40\n",
        "    lr: float = 0.0002\n",
        "\n",
        "    img_size = DefaultConfig.img_size\n",
        "\n",
        "    folder = '/kaggle/working/' #folder_name\n",
        "\n",
        "    # -------------------\n",
        "    verbose = True\n",
        "    verbose_step = 1\n",
        "    # -------------------\n",
        "\n",
        "    # --------------------\n",
        "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
        "    validation_scheduler = False  # do scheduler.step after validation stage loss\n",
        "\n",
        "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
        "#     scheduler_params = dict(\n",
        "#         max_lr=0.001,\n",
        "#         epochs=n_epochs,\n",
        "#         steps_per_epoch=int(len(train_dataset) / batch_size),\n",
        "#         pct_start=0.1,\n",
        "#         anneal_strategy='cos',\n",
        "#         final_div_factor=10**5\n",
        "#     )\n",
        "\n",
        "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "    scheduler_params = dict(\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=1,\n",
        "        verbose=False,\n",
        "        threshold=0.0001,\n",
        "        threshold_mode='abs',\n",
        "        cooldown=0,\n",
        "        min_lr=1e-8,\n",
        "        eps=1e-08\n",
        "    )\n",
        "    # --------------------"
      ],
      "metadata": {
        "id": "A1ef-j0G6EyW"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "CnkGRzm46GeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNNDetector(torch.nn.Module):\n",
        "    def __init__(self, pretrained=False, **kwargs):\n",
        "        super(FasterRCNNDetector, self).__init__()\n",
        "        # load pre-trained model incl. head\n",
        "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained, pretrained_backbone=pretrained)\n",
        "\n",
        "        # get number of input features for the classifier custom head\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "        # replace the pre-trained head with a new one\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, DefaultConfig.num_classes)\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        return self.model(images, targets)\n",
        "import gc\n",
        "\n",
        "def get_model(checkpoint_path=None, pretrained=False):\n",
        "    model = FasterRCNNDetector(pretrained=pretrained)\n",
        "\n",
        "    # Load the trained weights\n",
        "    if checkpoint_path is not None:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        del checkpoint\n",
        "        gc.collect()\n",
        "\n",
        "    return model.cuda()\n",
        "\n",
        "net = get_model(pretrained=True)\n"
      ],
      "metadata": {
        "id": "r548h3yY6HVB"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def run_training(fold=0):\n",
        "    net.to(device)\n",
        "\n",
        "    train_dataset = get_train_dataset(fold_number=fold)\n",
        "    train_data_loader = get_train_data_loader(\n",
        "        train_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size\n",
        "    )\n",
        "\n",
        "    validation_dataset = get_validation_dataset(fold_number=fold)\n",
        "    validation_data_loader = get_validation_data_loader(\n",
        "        validation_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size\n",
        "    )\n",
        "\n",
        "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
        "    fitter.fit(train_data_loader, validation_data_loader)"
      ],
      "metadata": {
        "id": "TfP7UwV-6h-w"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple inference\n",
        "validation_dataset = get_validation_dataset(fold_number=DefaultConfig.fold_num)\n",
        "validation_data_loader = get_validation_data_loader(\n",
        "    validation_dataset,\n",
        "    batch_size=TrainGlobalConfig.batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "sues9mOn6gHF"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    images, targets, image_id = next(iter(validation_data_loader))\n",
        "\n",
        "    images = list(img.to(device) for img in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    # Check if the list is not empty\n",
        "    if images and targets:\n",
        "        boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)  # Assuming you want the first element\n",
        "        sample = images[0].permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "        # Check if the image is not None\n",
        "        if sample is not None:\n",
        "            for box in boxes:\n",
        "                cv2.rectangle(sample,\n",
        "                              (box[0], box[1]),\n",
        "                              (box[2], box[3]),\n",
        "                              (255, 0, 0), 3)\n",
        "\n",
        "            # Your plotting code here\n",
        "            plt.imshow(sample)\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Error: Image is None.\")\n",
        "    else:\n",
        "        print(\"Error: Empty list of images or targets.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAqAe8mC6kgN",
        "outputId": "6472440e-e515-4a47-80bb-6a3198519bf5"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: Caught AttributeError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"<ipython-input-72-c97abc195603>\", line 15, in __getitem__\n",
            "    image, boxes, labels = self.load_image_and_boxes(index)\n",
            "  File \"<ipython-input-72-c97abc195603>\", line 41, in load_image_and_boxes\n",
            "    image = cv2.imread(self.file_names[index], cv2.IMREAD_COLOR).copy().astype(np.float32)\n",
            "AttributeError: 'NoneType' object has no attribute 'copy'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_model('../input/coviddetection-temp/best-checkpoint-005epoch.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "QfZYQNcS7O2e",
        "outputId": "051e2958-dbb6-4130-fde1-508b4cfa52ed"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-2d14d6fac083>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/coviddetection-temp/best-checkpoint-005epoch.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-83-f39895c31037>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(checkpoint_path, pretrained)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Load the trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/coviddetection-temp/best-checkpoint-005epoch.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "outputs = net(images)\n",
        "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "KZVCEP9i6uXP",
        "outputId": "bff66567-354e-475a-d649-a02a7d60fc2b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-94db4eb77234>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcpu_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_device\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "for box in boxes:\n",
        "    cv2.rectangle(sample,\n",
        "                  (box[0], box[1]),\n",
        "                  (box[2], box[3]),\n",
        "                  (220, 0, 0), 2)\n",
        "\n",
        "ax.set_axis_off()\n",
        "ax.imshow(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "HmaC-eop67TU",
        "outputId": "5aba13aa-b6fe-42ae-c1d9-c135f837fd5d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-b48cb9f1885c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     cv2.rectangle(sample,\n\u001b[1;32m      5\u001b[0m                   \u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'boxes' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}